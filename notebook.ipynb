{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#file path to CSV\n",
    "file_path = 'Reviews.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)  \n",
    "\n",
    "df.to_csv('Reviews_full.csv', index=False)# Save the DataFrame to a new CSV file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(5))  # Display the columns in the DataFrame+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)  # Display the columns in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary columns \n",
    "columns_to_drop = ['Id', 'ProductId','UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator','Time']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(df.columns)  # Display the first 5 rows of the DataFrame after dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "Loops dont work as expected in pandas \n",
    "since it uses vectorized operations \n",
    "so accessing and modifying a row in a loop is not efficient.\n",
    "Instead, we can use the str.lower() method directly on the column.\n",
    "and that will convert every row in column\n",
    "\"\"\"\n",
    "# Convert all text in 'Summary' and 'Text' columns to lowercase\n",
    "df['Summary'] = df['Summary'].str.lower() \n",
    "df['Text'] = df['Text'].str.lower()  \n",
    "# Convert each text to lowercase\n",
    "    \n",
    "print(df.head(5))  \n",
    "# Display the first 5 rows of the DataFrame after converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\"\"\"\n",
    "Got typed error: since the 'Summary' and 'Text' columns are not strings,\n",
    "we need to convert them to strings before applying regex.\n",
    "or use str.replace \n",
    "\"\"\"\n",
    "\n",
    "df['Summary'] = df['Summary'].str.replace(f\"[{re.escape(string.punctuation)}]\", \"\", regex=True)\n",
    "df['Text'] = df['Text'].str.replace(f\"[{re.escape(string.punctuation)}]\", \"\", regex=True)\n",
    "\n",
    "print(df.head(5))  # Display the first 5 rows after removing punctuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine 'Summary' and 'Text' columns into a new column \n",
    "\n",
    "df['FullReview'] = df['Summary'].astype(str) + ' ' + df['Text'].astype(str)\n",
    "\n",
    "#replace multiple whitespaces with a single whitespace\n",
    "df['FullReview'] = df['FullReview'].str.replace(r'\\s+', ' ', regex=True)  \n",
    "\n",
    "#drop  Summary and Text columns\n",
    "df = df.drop(columns=['Summary', 'Text']) \n",
    "\n",
    "print(df['FullReview'].head(5)) \n",
    "print(df.columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stop words are words like \n",
    "a,the,and,that\n",
    "they dont add meaning to text,so removing reduces noise \n",
    "\"\"\"\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')  # Download stopwords if not already downloaded\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Split text into words\n",
    "    filtered = [w for w in words if w not in stopwords]\n",
    "    return ' '.join(filtered)  # Join words back into a string\n",
    "\n",
    "# Apply the remove_stopwords function to the 'FullReview' column\n",
    "df['FullReview'] = df['FullReview'].apply(remove_stopwords)\n",
    "print(df['FullReview'].head(5))  # Display the first 5 rows after removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Normally would need to tokenize \n",
    "but TF-IDF work with raw text and tokenizes automatically\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#limit the number of unique words to 10000, max_df=0.8 means ignore words that appear in more than 80% of the documents\n",
    "tfidf = TfidfVectorizer(max_features=80000,max_df=0.8)\n",
    "#return a sparse matrix of TF-IDF features\n",
    "\n",
    "X = tfidf.fit_transform(df['FullReview'])\n",
    "#print shape of matrix so number of reviews and number of unique words\n",
    "print(X.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the DataFrame \n",
    "#delete row with null values \n",
    "for col in df.columns:\n",
    "    if df[col].isnull().any():\n",
    "        print(f\"Column '{col}' has null values. Dropping rows with null values.\")\n",
    "        df = df.dropna(subset=[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_sentiment(score):\n",
    "    if score in [4, 5]:\n",
    "        return 'Positive'\n",
    "    elif score == 3:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "df['Sentiment'] = df['Score'].apply(label_sentiment)\n",
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,log_loss\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\"\"\"using Linear Regression\n",
    "    since it assumes correlation between features and target variable\n",
    "    we are trying to predict what a review rating would be\n",
    "    based on text in the review \n",
    "\"\"\"\n",
    "#df['Sentiment'] = df['Score'].apply(lambda x: 1 if x >= 3 else 0)\n",
    "  \n",
    "def run_model(df, X_tfidf):\n",
    "    # using the TF-IDF scores for X, because depending on the score of the tfidf the model will be able to predict if the word likely to correlate with a score \n",
    "    X = X_tfidf\n",
    "    y = df['Sentiment']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs',class_weight={'Positive': 1, 'Neutral': 5, 'Negative': 3})\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred), \"\\n\")\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred), \"\\n\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    loss = log_loss(y_test, y_pred_proba)\n",
    "    print(\"Log Loss:\", loss, \"\\n\")\n",
    "    with open(\"results.txt\", \"a\") as f:\n",
    "        f.write(\"Final model run 88%\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\" + str(confusion_matrix(y_test, y_pred)) + \"\\n\")\n",
    "        f.write(\"Accuracy Score: \\n\" + str(accuracy_score(y_test, y_pred)) + \"\\n\")\n",
    "        f.write(\"Classification Report:\\n\" + classification_report(y_test, y_pred) + \"\\n\")\n",
    "        f.write(\"Log Loss: \" + str(loss) + \"\\n\")\n",
    "        f.close()\n",
    "    # Save the model to a file\n",
    "    joblib.dump(model, 'sentiment_model.pkl')\n",
    "    return X_train, X_test, y_train, y_test,model\n",
    "    \n",
    "    \n",
    "print(\"Starting Testing\")    \n",
    "run_model(df, X)\n",
    "# Call the function to perform train-test split and model training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
